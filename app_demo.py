import gradio as gr
import torch
from unsloth import FastLanguageModel

# åŠ è½½æ¨¡å‹ (å‡è®¾ä½ å·²ç»è®­ç»ƒå¹¶ä¿å­˜äº† lora_model)
# æ³¨æ„ï¼šå¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œè¿™é‡Œéœ€è¦æŒ‡å‘ base modelï¼Œå¹¶åŠ è½½ adapter
# ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œè¿™é‡Œåªå†™æ¨ç†é€»è¾‘ç»“æ„

def load_model():
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # è¿™é‡Œå¡«ä½ ä¿å­˜çš„ç›®å½•
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)
    return model, tokenizer

# æç¤ºè¯æ¨¡ç‰ˆ
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def chat(message, history):
    # è¿™é‡Œåº”è¯¥æ”¾ç½®çœŸæ­£çš„æ¨¡å‹æ¨ç†ä»£ç 
    # ä¸ºäº†é˜²æ­¢ä½ åœ¨æ²¡æœ‰GPUçš„æœ¬åœ°ç¯å¢ƒæŠ¥é”™ï¼Œè¿™é‡Œå†™ä¸€ä¸ªæ¨¡æ‹Ÿè¿”å›
    # å®é™…éƒ¨ç½²æ—¶å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    
    """
    inputs = tokenizer(
        [alpaca_prompt.format(message, "", "")], 
        return_tensors = "pt"
    ).to("cuda")
    
    outputs = model.generate(**inputs, max_new_tokens = 128)
    response = tokenizer.batch_decode(outputs)[0]
    return response.split("### Response:\\n")[-1].replace("<|end_of_text|>", "")
    """
    
    return f"[Mock Output] Model received: {message}. (Run on GPU to see real inference)"

# åˆ›å»ºç•Œé¢
iface = gr.ChatInterface(
    fn=chat,
    title="ğŸ§¬ LifeOS Assistant",
    description="Ask me about your schedule, health protocols, or diet.",
    examples=["What is the plan for Tuesday?", "My shoulder hurts.", "I slept poorly last night."],
    theme="soft"
)

if __name__ == "__main__":
    iface.launch()